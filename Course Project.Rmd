---
title: "Course Project"
author: "David N. Cohron"
date: "1/17/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#  load needed libraries
library(caret)
library(randomForest)
```

##  This is the markdown file for the Coursera Johns Hopkins University Practical Machine Learning course

## Load the dataset

The first task is to load the data.  Having worked with this dataset for a bit, I already know that the dataset is sparse and that many missing values need to be converted to 'na' values and I will do that on the read from file.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# create path to dataset via web
trainPathWeb <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testPathWeb <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

#  load dataset
#  set NA, #DIV/0! and blank space to na
pmlData <- read.csv(file = trainPathWeb, header = TRUE, sep = ",", na.strings = c("NA", "#DIV/0!", ""))
testing <- read.csv(file = testPathWeb, header = TRUE, sep = ",", na.strings = c("NA", "#DIV/0!", ""))

```

## Data Exploration

Lets begin with examining the dataset. Dimensions are:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
dim(pmlData)

```

Head of the data is:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
head(pmlData)

```


## Data Transformation

From working with the dataset, I know it needs three transformations in order to be ready to apply machine learning algorithms:

1)  Need to remove the columns that are mostly NA values.

2)  Remove predictors that are unchanging, and thus not useful for analysis.  An inspection of
the dataset shows a real bifurcation of columns that change and those that are almost completely static. See: https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/

3)  The first column (ID) throws off the algorithms.  Almost any machine learning algorithm will take the ID and, because the dataset is sorted, will yield a prediction of all "A" for the test set because they are single observations.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# 1) Remove columns with mostly NA values
# Find columns with many NA values
naCount <-sapply(pmlData, function(y) sum(length(which(is.na(y)))))

# remove columns with too many missing values
# set removal threshold at 75% or greater
threshold <- .75
naList <- naCount[naCount >= threshold * dim(pmlData[1])]
namesNA <- names(pmlData) %in% names(naList)
pmlData <- pmlData[!namesNA]

# 2) Remove low variance predictors
# look for variables that do not change/ pseudo-constant
unchangingData <- nearZeroVar(pmlData, saveMetrics = TRUE)
subsetNZV <- unchangingData[unchangingData$nzv == TRUE, ]

# remove NZV columns
namesNZV <- names(pmlData) %in% as.list(row.names(subsetNZV))
pmlData <- pmlData[!namesNZV]

# 3) Remove first column (ID number)
# which prior runs showed skewed the prediction results
pmlData <- pmlData[c(-1)]
```

Which leaves us with a reduced dataset ready for some machine learning. Dimensions are:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
dim(pmlData)

```

Head of the data now is:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
head(pmlData)

```


## Segment dataset for model use

Segment data into "training" and "validation" sets as testing set is given. 
Dimensions of the training and validating sets are, respectively:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#  split data into training and validation sets
inTrain = createDataPartition(pmlData$classe, p = 3/4)[[1]]
training = pmlData[ inTrain,]
validating = pmlData[-inTrain,]

dim(training)
dim(validating)
```


## Model Creation and Results

Run train function on the training dataset using random forest.  I was going to build a stacked model but found the random forest to be so accurate that I did not get any appreciable increase in accuracy using the other models.  So then use that model to make predictions on the validation set.  This takes a good bit of time to run as the data set is so large.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#  set seed for reproduceability of results
set.seed(12345)

#  fit models
modFitRF <- train(classe~., method="rf",  data = training )

#  make predictions
predRF <- predict(modFitRF, validating)

```

##  Cross validation and out-of-sample error estimation
Lets look at the summary confusion matrix to see how the model faired in prediction on the validation dataset:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# look at accuracy of results
confusionMatrix(predRF, validating$classe)

```


## Show an example graph of decision tree generated by model

Lets look at the dendogram of the tree.  Have to use a different package to generate and plot tree.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# plot random forest
library(partykit)
y <- ctree(classe~., data = training)
class(y)

plot(y, gp = gpar(fontsize = 6),     # font size changed to 6  
     inner_panel=node_inner,
     ip_args=list(
       abbreviate = TRUE, 
       id = FALSE)
     )

```